{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMp095VsKZAESdlwHKPW1CF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/singhraj00/langchain-tutorial/blob/main/LangChainPrompts.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **LangChain Prompts**\n",
        "\n",
        "![](https://i.ytimg.com/vi/3TGqlQxpuU0/maxresdefault.jpg)\n",
        "\n",
        "## **Prompts** are the input instructions or queries given to a model to guide its output."
      ],
      "metadata": {
        "id": "yBmpi1tXPi-x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install LangChain"
      ],
      "metadata": {
        "id": "K8IzEihHXA17"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain\n",
        "!pip install langchain_core"
      ],
      "metadata": {
        "id": "1M9Oak_sxCZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GroQ API Integration"
      ],
      "metadata": {
        "id": "1E_ox8kMPLYv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-groq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mqJ9Hv6YWaQP",
        "outputId": "1f4f5c45-be75-47c9-c026-f4c571fa3bb7",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-groq\n",
            "  Downloading langchain_groq-0.3.1-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting langchain-core<1.0.0,>=0.3.47 (from langchain-groq)\n",
            "  Using cached langchain_core-0.3.47-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting groq<1,>=0.4.1 (from langchain-groq)\n",
            "  Downloading groq-0.20.0-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain-groq) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain-groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain-groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain-groq) (2.10.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain-groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain-groq) (4.12.2)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.47->langchain-groq) (0.3.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.47->langchain-groq) (9.0.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.47->langchain-groq) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.47->langchain-groq) (6.0.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.47->langchain-groq) (24.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->groq<1,>=0.4.1->langchain-groq) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.47->langchain-groq) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.47->langchain-groq) (3.10.15)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.47->langchain-groq) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.47->langchain-groq) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.47->langchain-groq) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain-groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain-groq) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.47->langchain-groq) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.47->langchain-groq) (2.3.0)\n",
            "Downloading langchain_groq-0.3.1-py3-none-any.whl (15 kB)\n",
            "Downloading groq-0.20.0-py3-none-any.whl (124 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.9/124.9 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.3.47-py3-none-any.whl (417 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m417.1/417.1 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: groq, langchain-core, langchain-groq\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.45\n",
            "    Uninstalling langchain-core-0.3.45:\n",
            "      Successfully uninstalled langchain-core-0.3.45\n",
            "Successfully installed groq-0.20.0 langchain-core-0.3.47 langchain-groq-0.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Langchain GroQ Model Integration"
      ],
      "metadata": {
        "id": "yyvjnNZfTJ9K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "anNEz8wIPdj3",
        "outputId": "c6fb8e95-5eff-49cf-b73c-d78b8cc3424d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Large Language Models (LLMs) like myself store facts about data through a combination of natural language processing (NLP) and machine learning techniques. Here's a simplified overview of how we store facts:\n",
            "\n",
            "1. **Tokenization**: When we receive text data, we break it down into smaller units called tokens. These tokens can be words, characters, or even subwords (smaller units within words).\n",
            "2. **Embeddings**: We create vector representations of these tokens, called embeddings, which capture their semantic meaning. These embeddings are learned during training and are based on the context in which the tokens appear.\n",
            "3. **Knowledge Graph**: As we process more text data, we build a massive knowledge graph that represents the relationships between tokens, entities, and concepts. This graph is a complex network of nodes and edges that store information about the world.\n",
            "4. **Entity Recognition**: We identify and extract specific entities from the text, such as names, locations, organizations, and dates. These entities are then linked to their corresponding nodes in the knowledge graph.\n",
            "5. **Relationship Extraction**: We extract relationships between entities, such as \"Person A is a employee of Company B\" or \"Event C occurred on Date D\". These relationships are stored as edges in the knowledge graph.\n",
            "6. **Graph Updates**: As we receive new text data, we update the knowledge graph by adding new nodes, edges, and relationships. This process is called graph updating or knowledge graph embedding.\n",
            "7. **Memory-Augmented Attention**: To store and retrieve information efficiently, we use a mechanism called memory-augmented attention. This allows us to focus on specific parts of the knowledge graph when generating text or answering questions.\n",
            "8. **Sparse and Dense Representations**: We store the knowledge graph in a combination of sparse and dense representations. Sparse representations are used for efficient storage and retrieval, while dense representations are used for complex reasoning and inference.\n",
            "\n",
            "Some key techniques used to store facts in LLMs include:\n",
            "\n",
            "* **Transformers**: A type of neural network architecture that's particularly well-suited for NLP tasks.\n",
            "* **Self-Attention**: A mechanism that allows the model to attend to different parts of the input sequence when generating text or answering questions.\n",
            "* **Graph Neural Networks**: A type of neural network designed to work directly with graph-structured data.\n",
            "* **Knowledge Distillation**: A technique used to transfer knowledge from a large pre-trained model to a smaller model.\n",
            "\n",
            "By combining these techniques, LLMs can store and retrieve vast amounts of information about the world, allowing us to generate human-like text, answer questions, and even create new content.\n"
          ]
        }
      ],
      "source": [
        "from google.colab import userdata\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "\n",
        "API_KEY = userdata.get('GROQ_API_KEY')\n",
        "\n",
        "chat = ChatGroq(model_name='llama-3.3-70b-versatile',api_key=API_KEY,temperature=0.5)\n",
        "\n",
        "system = 'you are a helpful assistant'\n",
        "human = '{text}'\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([(\"system\",system),(\"human\",human)])\n",
        "\n",
        "chain = prompt | chat\n",
        "\n",
        "result = chain.invoke({'text':'Explain the how think llm to store facts about data.'})\n",
        "\n",
        "print(result.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LangChain Prompts\n",
        "\n",
        "### **Prompts** are the input instructions or queries given to a model to guide its output.\n",
        "\n",
        "``model.invoke('what is prompts in langchain.')``\n",
        "\n",
        "## Above, Inside invoke method we have written a prompt to given model."
      ],
      "metadata": {
        "id": "0edbF4PoTcB7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Static Vs Dynamic Prompts\n",
        "\n",
        "### static prompts are fixed and pre-defined, while dynamic prompts adapt in real-time based on user input or context."
      ],
      "metadata": {
        "id": "9rYKwUtlUobK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Static Prompt Example"
      ],
      "metadata": {
        "id": "1i9nCWThXU3B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile static_app.py\n",
        "from langchain_groq import ChatGroq\n",
        "from  google.colab import userdata\n",
        "import streamlit as st\n",
        "\n",
        "# API_KEY = userdata.get('GROQ_API_KEY')\n",
        "\n",
        "chat = ChatGroq(model_name='llama-3.3-70b-versatile',api_key='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',temperature=0.5)\n",
        "\n",
        "st.header('Research Assitant Tool')\n",
        "\n",
        "## static prompt\n",
        "user_input = st.text_input('Enter your prompt')\n",
        "\n",
        "if st.button(\"summarize\"):\n",
        "  res = chat.invoke(user_input)\n",
        "  st.write(res.content)\n"
      ],
      "metadata": {
        "id": "1dQx6bPWb8p1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a779b4ef-5016-4e06-eb64-51ca5f4db801"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install streamlit -q\n",
        "!wget -q -O - ipv4.icanhazip.com\n",
        "! streamlit run static_app.py & npx localtunnel --port 8501"
      ],
      "metadata": {
        "id": "vFbTxrzva3sk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dynamic Prompt Example"
      ],
      "metadata": {
        "id": "Ixi1EGTaXgOV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile dynamic_app.py\n",
        "from langchain_groq import ChatGroq\n",
        "from  google.colab import userdata\n",
        "import streamlit as st\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "\n",
        "# API_KEY = userdata.get('GROQ_API_KEY')\n",
        "\n",
        "\n",
        "st.header('Research Assitant Tool')\n",
        "\n",
        "paper_choice = [\"Attention is all you need\",\"Bert: Pre-training of Deep Bidirectional transformers\",\"GPT-3: Language models are few-shot learners\",\"Diffusion models beat GANs on Image Synthesis\"]\n",
        "style_choice = ['Bedgginer-Friendly','Technical','Code-Oriented','Mathematical']\n",
        "length_choice = ['short (1-2 paragraphs)','Medium (1-5 paragraphs)','Long (detailed explaination)']\n",
        "\n",
        "paper_input = st.selectbox(\"Select Research Paper Name\",paper_choice)\n",
        "style_input = st.selectbox(\"Select Explaination Style\",style_choice)\n",
        "length_input = st.selectbox(\"Select Explaination Length\",length_choice)\n",
        "\n",
        "\n",
        "chat = ChatGroq(model_name='llama-3.3-70b-versatile',api_key='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',temperature=0.5)\n",
        "\n",
        "\n",
        "## Dynamic Template Example\n",
        "\n",
        "SYSTEM_TEMPLATE =\"\"\"\n",
        "Please summarize the research paper titled \"{paper_input}\"\n",
        "with the following specifications:\n",
        "Explaination style: {style_input}\n",
        "Explaination length: {length_input}\n",
        "\n",
        "1. Mathematical Details:\n",
        "- Include relevant mathematical equations if present in the paper.\n",
        "- Explain the mathematical concepts using simple, intutive code snippets where applicable.\n",
        "\n",
        "2. Analogies:\n",
        "\n",
        "- Use relatable analogies to simplify complex ideas.\n",
        "\n",
        "If certain information is not available in the paper, responds with \"insuffiecient information available\" instead of guessing.\n",
        "\n",
        "Ensure the summary is clear, accurate, and concise and aligned with provided style and length.\n",
        "\"\"\"\n",
        "\n",
        "template = PromptTemplate(template=SYSTEM_TEMPLATE,\n",
        "                          input_variables=['paper_input',\n",
        "                                           'style_input',\n",
        "                                           'length_input'],\n",
        "                          ## default validation\n",
        "                          validate_template=True\n",
        "                          ## validate input variables length\n",
        "                         )\n",
        "\n",
        "\n",
        "prompt = template.invoke({\n",
        "    'paper_input':paper_input,\n",
        "    'style_input':style_input,\n",
        "    'length_input':length_input\n",
        "})\n",
        "\n",
        "if st.button(\"summarize\"):\n",
        "  res = chat.invoke(prompt)\n",
        "  st.write(res.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DhNdKG6GXfxu",
        "outputId": "eb9327a5-b17a-43bb-90d8-7e931e342604"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting dynamic_app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompt Template\n",
        "\n",
        "A Prompt Templagte in LangChain is a structured way to create prompts dynamically by inserting variables into a predefined template. Instead of hardcoding prompts, **PromptTemplate** allows you to define placeholders that can be filled in at runtime with different inputs.\n",
        "\n",
        "This makes it reusable, flexible, and easy to manage, especially when working with dynamic user inputs or automated workflows.\n",
        "\n",
        "## Why use PromptTemplate over fstrings ?\n",
        "- Default Validation\n",
        "- Reusable\n",
        "- LangChain Ecosystem"
      ],
      "metadata": {
        "id": "76ZsE8-9eF32"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Default Validation"
      ],
      "metadata": {
        "id": "S_E-xzgIjb50"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile dynamic_app.py\n",
        "from langchain_groq import ChatGroq\n",
        "from  google.colab import userdata\n",
        "import streamlit as st\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "\n",
        "# API_KEY = userdata.get('GROQ_API_KEY')\n",
        "\n",
        "\n",
        "st.header('Research Assitant Tool')\n",
        "\n",
        "paper_choice = [\"Attention is all you need\",\"Bert: Pre-training of Deep Bidirectional transformers\",\"GPT-3: Language models are few-shot learners\",\"Diffusion models beat GANs on Image Synthesis\"]\n",
        "style_choice = ['Bedgginer-Friendly','Technical','Code-Oriented','Mathematical']\n",
        "length_choice = ['short (1-2 paragraphs)','Medium (1-5 paragraphs)','Long (detailed explaination)']\n",
        "\n",
        "paper_input = st.selectbox(\"Select Research Paper Name\",paper_choice)\n",
        "style_input = st.selectbox(\"Select Explaination Style\",style_choice)\n",
        "length_input = st.selectbox(\"Select Explaination Length\",length_choice)\n",
        "\n",
        "\n",
        "chat = ChatGroq(model_name='llama-3.3-70b-versatile',api_key='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',temperature=0.5)\n",
        "\n",
        "\n",
        "## Dynamic Template Example\n",
        "\n",
        "SYSTEM_TEMPLATE =\"\"\"\n",
        "Please summarize the research paper titled \"{paper_input}\"\n",
        "with the following specifications:\n",
        "Explaination style: {style_input}\n",
        "Explaination length: {length_input}\n",
        "\n",
        "1. Mathematical Details:\n",
        "- Include relevant mathematical equations if present in the paper.\n",
        "- Explain the mathematical concepts using simple, intutive code snippets where applicable.\n",
        "\n",
        "2. Analogies:\n",
        "\n",
        "- Use relatable analogies to simplify complex ideas.\n",
        "\n",
        "If certain information is not available in the paper, responds with \"insuffiecient information available\" instead of guessing.\n",
        "\n",
        "Ensure the summary is clear, accurate, and concise and aligned with provided style and length.\n",
        "\"\"\"\n",
        "\n",
        "template = PromptTemplate(template=SYSTEM_TEMPLATE,\n",
        "                          input_variables=['paper_input',\n",
        "                                           'style_input',\n",
        "                                           'length_input'],\n",
        "                          ## default validation\n",
        "                          validate_template=True\n",
        "                          ## validate input variables length\n",
        "                         )\n",
        "\n",
        "\n",
        "prompt = template.invoke({\n",
        "    'paper_input':paper_input,\n",
        "    'style_input':style_input,\n",
        "    'length_input':length_input\n",
        "})\n",
        "\n",
        "if st.button(\"summarize\"):\n",
        "  res = chat.invoke(prompt)\n",
        "  st.write(res.content)\n"
      ],
      "metadata": {
        "id": "sRj91AjzjZpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Reusability\n",
        "  - Saving Prompts\n",
        "  - Use Downloaded Prompts\n",
        "\n",
        "# Saving Prompts"
      ],
      "metadata": {
        "id": "3LPG1thBjjVX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "template =\"\"\"\n",
        "Please summarize the research paper titled \"{paper_input}\"\n",
        "with the following specifications:\n",
        "Explaination style: {style_input}\n",
        "Explaination length: {length_input}\n",
        "\n",
        "1. Mathematical Details:\n",
        "- Include relevant mathematical equations if present in the paper.\n",
        "- Explain the mathematical concepts using simple, intutive code snippets where applicable.\n",
        "\n",
        "2. Analogies:\n",
        "\n",
        "- Use relatable analogies to simplify complex ideas.\n",
        "\n",
        "If certain information is not available in the paper, responds with \"insuffiecient information available\" instead of guessing.\n",
        "\n",
        "Ensure the summary is clear, accurate, and concise and aligned with provided style and length.\n",
        "\"\"\"\n",
        "\n",
        "template = PromptTemplate(template=template,\n",
        "                          input_variables=['paper_input','style_input','length_input'],\n",
        "\n",
        "                          ## default validation\n",
        "                          validate_template=True\n",
        "                          ## validate input variables length\n",
        "                         )\n",
        "\n",
        "template.save('prompt_template.json')"
      ],
      "metadata": {
        "id": "O8USRDj5V9OX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#  Use Downloaded Template"
      ],
      "metadata": {
        "id": "rP0QJvwUg2L7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile dynamic_app.py\n",
        "from langchain_groq import ChatGroq\n",
        "from  google.colab import userdata\n",
        "import streamlit as st\n",
        "from langchain_core.prompts import load_prompt\n",
        "\n",
        "# API_KEY = userdata.get('GROQ_API_KEY')\n",
        "\n",
        "\n",
        "st.header('Research Assitant Tool')\n",
        "\n",
        "paper_choice = [\"Attention is all you need\",\"Bert: Pre-training of Deep Bidirectional transformers\",\"GPT-3: Language models are few-shot learners\",\"Diffusion models beat GANs on Image Synthesis\"]\n",
        "style_choice = ['Bedgginer-Friendly','Technical','Code-Oriented','Mathematical']\n",
        "length_choice = ['short (1-2 paragraphs)','Medium (1-5 paragraphs)','Long (detailed explaination)']\n",
        "\n",
        "paper_input = st.selectbox(\"Select Research Paper Name\",paper_choice)\n",
        "style_input = st.selectbox(\"Select Explaination Style\",style_choice)\n",
        "length_input = st.selectbox(\"Select Explaination Length\",length_choice)\n",
        "\n",
        "\n",
        "chat = ChatGroq(model_name='llama-3.3-70b-versatile',api_key='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',temperature=0.5)\n",
        "\n",
        "\n",
        "## Dynamic Template Example\n",
        "\n",
        "template = load_prompt('/content/prompt_template.json')\n",
        "\n",
        "\n",
        "prompt = template.invoke({\n",
        "    'paper_input':paper_input,\n",
        "    'style_input':style_input,\n",
        "    'length_input':length_input\n",
        "})\n",
        "\n",
        "if st.button(\"summarize\"):\n",
        "  res = chat.invoke(prompt)\n",
        "  st.write(res.content)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O376obccgVWm",
        "outputId": "701f7f11-5d91-4a98-cd0a-5dea28b5d7de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting dynamic_app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Ecosystem : PromptsTemplate Tightly Coupled with Chains"
      ],
      "metadata": {
        "id": "1KPIbgTljBst"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile dynamic_app.py\n",
        "from langchain_groq import ChatGroq\n",
        "from  google.colab import userdata\n",
        "import streamlit as st\n",
        "from langchain_core.prompts import load_prompt\n",
        "\n",
        "# API_KEY = userdata.get('GROQ_API_KEY')\n",
        "\n",
        "\n",
        "st.header('Research Assitant Tool')\n",
        "\n",
        "paper_choice = [\"Attention is all you need\",\"Bert: Pre-training of Deep Bidirectional transformers\",\"GPT-3: Language models are few-shot learners\",\"Diffusion models beat GANs on Image Synthesis\"]\n",
        "style_choice = ['Bedgginer-Friendly','Technical','Code-Oriented','Mathematical']\n",
        "length_choice = ['short (1-2 paragraphs)','Medium (1-5 paragraphs)','Long (detailed explaination)']\n",
        "\n",
        "paper_input = st.selectbox(\"Select Research Paper Name\",paper_choice)\n",
        "style_input = st.selectbox(\"Select Explaination Style\",style_choice)\n",
        "length_input = st.selectbox(\"Select Explaination Length\",length_choice)\n",
        "\n",
        "\n",
        "chat = ChatGroq(model_name='llama-3.3-70b-versatile',api_key='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',temperature=0.5)\n",
        "\n",
        "\n",
        "## Dynamic Template Example\n",
        "\n",
        "template = load_prompt('/content/prompt_template.json')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if st.button(\"summarize\"):\n",
        "  ## create an chain ecosystem\n",
        "  chain = template | chat\n",
        "  res = chain.invoke({\n",
        "    'paper_input':paper_input,\n",
        "    'style_input':style_input,\n",
        "    'length_input':length_input\n",
        "    })\n",
        "  st.write(res.content)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QNyccbktj3z-",
        "outputId": "f8c257e8-dc20-4304-c9c7-a8a80099197e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting dynamic_app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install streamlit -q\n",
        "!wget -q -O - ipv4.icanhazip.com\n",
        "! streamlit run dynamic_app.py & npx localtunnel --port 8501"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "maC9KesdhCTd",
        "outputId": "cb5a51ef-b64b-4cd4-f6c9-34fb3ad38443"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34.145.75.97\n",
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.145.75.97:8501\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0Kyour url is: https://tiny-kids-worry.loca.lt\n",
            "\u001b[34m  Stopping...\u001b[0m\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Let's Create an ChatBots !!"
      ],
      "metadata": {
        "id": "uPWu-W4Sly3c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_groq import ChatGroq\n",
        "from google.colab import userdata\n",
        "\n",
        "API_KEY = userdata.get('GROQ_API_KEY')\n",
        "\n",
        "chat = ChatGroq(model_name='llama-3.3-70b-versatile',api_key=API_KEY,temperature=0)\n",
        "\n",
        "while True:\n",
        "  user_input = input('You: ')\n",
        "  if user_input == 'exit':\n",
        "    break\n",
        "  result = chat.invoke(user_input)\n",
        "  print('AI: ',result.content)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CX4vYFbJhKNZ",
        "outputId": "b08f75d9-7bf8-482b-b77a-b724ca6d4a4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You: Hi\n",
            "AI:  It's nice to meet you. Is there something I can help you with or would you like to chat?\n",
            "You: which number is greater 2 or 0\n",
            "AI:  The number 2 is greater than 0.\n",
            "You: now greater number multiply by 10\n",
            "AI:  I don't see any numbers provided. Could you please provide two numbers so I can determine the greater number and multiply it by 10?\n",
            "You: i also tell that who is greater use that;s number\n",
            "AI:  It seems like you're asking me to compare two numbers and determine which one is greater. However, you haven't provided the numbers yet. Please go ahead and share the numbers you'd like to compare, and I'll be happy to help you determine which one is greater.\n",
            "You: now multiply the bigger number by 10\n",
            "AI:  This conversation has just begun. I'm happy to help, but I don't have any numbers to work with yet. If you provide two numbers, I can identify the bigger one and multiply it by 10 for you.\n",
            "You: exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Here you noticed that, our chatbot does not remember previous chat."
      ],
      "metadata": {
        "id": "d7S5Zc_NnYy5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## now we have save the chat history"
      ],
      "metadata": {
        "id": "678PuMYlnjuM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_groq import ChatGroq\n",
        "from google.colab import userdata\n",
        "\n",
        "API_KEY = userdata.get('GROQ_API_KEY')\n",
        "\n",
        "chat = ChatGroq(model_name='llama-3.3-70b-versatile',api_key=API_KEY,temperature=0)\n",
        "\n",
        "chat_history = []\n",
        "\n",
        "while True:\n",
        "  user_input = input('You: ')\n",
        "  ## save user chats\n",
        "  chat_history.append(user_input)\n",
        "  if user_input == 'exit':\n",
        "    break\n",
        "  result = chat.invoke(user_input)\n",
        "  # save ai chats\n",
        "  chat_history.append(result.content)\n",
        "  print('AI: ',result.content)\n",
        "\n",
        "\n",
        "print(\"chat_history saved: \",chat_history)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d55ZoTy6ma4B",
        "outputId": "459376d8-f86e-49fa-b75d-70183d839bba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You: hi\n",
            "AI:  It's nice to meet you. Is there something I can help you with or would you like to chat?\n",
            "You: tell me capital of india\n",
            "AI:  The capital of India is **New Delhi**.\n",
            "You: where is india gate\n",
            "AI:  India Gate is located in New Delhi, the capital city of India. It is situated on the Rajpath, a grand boulevard that runs from the Rashtrapati Bhavan (the President's House) to the India Gate.\n",
            "\n",
            "The exact address of India Gate is:\n",
            "\n",
            "India Gate, Rajpath, New Delhi, Delhi 110001, India\n",
            "\n",
            "It is a prominent landmark and a popular tourist destination in Delhi, and is easily accessible by road, metro, or other modes of public transportation. The nearest metro station is the Central Secretariat Metro Station, which is about 1.5 kilometers away from India Gate.\n",
            "You: mohan goes to market and buy some fruits\n",
            "AI:  Mohan goes to the market and buys some fruits. He picks up a few apples, a bunch of bananas, and a dozen oranges. As he is about to leave, he notices a stall selling exotic fruits and decides to try some. He buys a few kiwis, a pineapple, and a watermelon. After making his purchases, Mohan heads home, excited to enjoy his fresh fruits. \n",
            "\n",
            "Would you like to know what Mohan does with the fruits when he gets home?\n",
            "You: shivam goes to market and buy some vegetables\n",
            "AI:  Shivam went to the market and bought some fresh vegetables, including carrots, broccoli, and tomatoes. He carefully selected the best ones and paid the vendor. As he was about to leave, he also picked up a few bunches of fresh coriander and a bag of potatoes. With his shopping done, Shivam headed back home, looking forward to cooking a delicious meal with the vegetables he had just bought. \n",
            "\n",
            "Would you like to know what Shivam cooked with the vegetables, or is there something else you'd like to know?\n",
            "You: now tell me which one buy fruits in market\n",
            "AI:  In a market, people typically buy fruits from:\n",
            "\n",
            "1. **Fruit vendors**: They are the ones who sell fruits, either from their own farms or sourced from other farmers.\n",
            "2. **Grocery stores**: Many grocery stores have a separate section for fruits, where customers can buy a variety of fruits.\n",
            "3. **Supermarkets**: Supermarkets usually have a large section dedicated to fruits, offering a wide range of options.\n",
            "4. **Street vendors**: In some areas, street vendors may also sell fruits, often at a lower price than traditional stores.\n",
            "5. **Farmers' markets**: Some markets are specifically dedicated to farmers selling their fresh produce, including fruits, directly to customers.\n",
            "\n",
            "These are the common places where people buy fruits in a market.\n",
            "You: now tell me which one perso mohan or shivam , buy fruits\n",
            "AI:  This conversation has just begun. I don't have any information about Perso Mohan or Shivam, or their activities. If you'd like to provide more context or details, I'd be happy to try and help.\n",
            "You: exit\n",
            "chat_history saved:  ['hi', \"It's nice to meet you. Is there something I can help you with or would you like to chat?\", 'tell me capital of india', 'The capital of India is **New Delhi**.', 'where is india gate', \"India Gate is located in New Delhi, the capital city of India. It is situated on the Rajpath, a grand boulevard that runs from the Rashtrapati Bhavan (the President's House) to the India Gate.\\n\\nThe exact address of India Gate is:\\n\\nIndia Gate, Rajpath, New Delhi, Delhi 110001, India\\n\\nIt is a prominent landmark and a popular tourist destination in Delhi, and is easily accessible by road, metro, or other modes of public transportation. The nearest metro station is the Central Secretariat Metro Station, which is about 1.5 kilometers away from India Gate.\", 'mohan goes to market and buy some fruits', 'Mohan goes to the market and buys some fruits. He picks up a few apples, a bunch of bananas, and a dozen oranges. As he is about to leave, he notices a stall selling exotic fruits and decides to try some. He buys a few kiwis, a pineapple, and a watermelon. After making his purchases, Mohan heads home, excited to enjoy his fresh fruits. \\n\\nWould you like to know what Mohan does with the fruits when he gets home?', 'shivam goes to market and buy some vegetables', \"Shivam went to the market and bought some fresh vegetables, including carrots, broccoli, and tomatoes. He carefully selected the best ones and paid the vendor. As he was about to leave, he also picked up a few bunches of fresh coriander and a bag of potatoes. With his shopping done, Shivam headed back home, looking forward to cooking a delicious meal with the vegetables he had just bought. \\n\\nWould you like to know what Shivam cooked with the vegetables, or is there something else you'd like to know?\", 'now tell me which one buy fruits in market', \"In a market, people typically buy fruits from:\\n\\n1. **Fruit vendors**: They are the ones who sell fruits, either from their own farms or sourced from other farmers.\\n2. **Grocery stores**: Many grocery stores have a separate section for fruits, where customers can buy a variety of fruits.\\n3. **Supermarkets**: Supermarkets usually have a large section dedicated to fruits, offering a wide range of options.\\n4. **Street vendors**: In some areas, street vendors may also sell fruits, often at a lower price than traditional stores.\\n5. **Farmers' markets**: Some markets are specifically dedicated to farmers selling their fresh produce, including fruits, directly to customers.\\n\\nThese are the common places where people buy fruits in a market.\", 'now tell me which one perso mohan or shivam , buy fruits', \"This conversation has just begun. I don't have any information about Perso Mohan or Shivam, or their activities. If you'd like to provide more context or details, I'd be happy to try and help.\", 'exit']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Problem: If you noticed **chat_history** there have too many chats, but how diiferentiate who's message send by User or AI.\n",
        "\n",
        "## Solution\n",
        "\n",
        "### LagChain Provides `Messages` concepts to differentiate messages System, User or AI.\n",
        "\n"
      ],
      "metadata": {
        "id": "8mXiZem5pcia"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import SystemMessage,HumanMessage,AIMessage\n",
        "from langchain_groq import ChatGroq\n",
        "from google.colab import userdata\n",
        "\n",
        "API_KEY = userdata.get('GROQ_API_KEY')\n",
        "\n",
        "chat = ChatGroq(model_name='llama-3.3-70b-versatile',api_key=API_KEY,temperature=0)\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"You are a helpful assistant\"),\n",
        "    HumanMessage(content='tell me about langchain')\n",
        "]\n",
        "\n",
        "res = chat.invoke(messages)\n",
        "\n",
        "messages.append(AIMessage(content=res.content))\n",
        "\n",
        "print(messages)"
      ],
      "metadata": {
        "id": "6M-p96qXn75G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cbda496-cd2e-4e62-f948-b61b17a4c6f2"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SystemMessage(content='You are a helpful assistant', additional_kwargs={}, response_metadata={}), HumanMessage(content='tell me about langchain', additional_kwargs={}, response_metadata={}), AIMessage(content='LangChain is an open-source framework designed to help developers build applications that utilize large language models (LLMs) more efficiently. It was created to simplify the process of integrating LLMs into various projects, making it easier for developers to focus on building their applications rather than worrying about the underlying infrastructure.\\n\\nHere are some key features and benefits of LangChain:\\n\\n1. **Modular architecture**: LangChain provides a modular architecture that allows developers to easily swap out different LLMs, agents, and tools, making it simple to experiment with different models and approaches.\\n2. **Agent-based architecture**: LangChain introduces the concept of \"agents,\" which are modular components that can be used to perform specific tasks, such as text generation, question-answering, or conversation management.\\n3. **LLM-agnostic**: LangChain is designed to be LLM-agnostic, meaning it can work with a wide range of large language models, including popular models like LLaMA, BERT, and RoBERTa.\\n4. **Simple and flexible API**: LangChain provides a simple and flexible API that makes it easy to integrate LLMs into applications, allowing developers to focus on building their application logic rather than worrying about the underlying LLM infrastructure.\\n5. **Support for multiple interfaces**: LangChain supports multiple interfaces, including text-based, voice-based, and visual interfaces, making it easy to build applications that interact with users in different ways.\\n6. **Extensive library of tools and agents**: LangChain comes with an extensive library of tools and agents that can be used to perform various tasks, such as text generation, sentiment analysis, and entity recognition.\\n7. **Community-driven**: LangChain is an open-source project with a growing community of developers and researchers contributing to its development, ensuring that it stays up-to-date with the latest advancements in LLMs and NLP.\\n\\nSome potential use cases for LangChain include:\\n\\n1. **Chatbots and conversational AI**: LangChain can be used to build conversational AI applications, such as chatbots, voice assistants, and customer service platforms.\\n2. **Text generation and summarization**: LangChain can be used to build applications that generate text, such as content generation, text summarization, and language translation.\\n3. **Sentiment analysis and opinion mining**: LangChain can be used to build applications that analyze sentiment and opinions, such as sentiment analysis, opinion mining, and social media monitoring.\\n4. **Question-answering and knowledge retrieval**: LangChain can be used to build applications that answer questions and retrieve knowledge, such as question-answering systems, knowledge graphs, and recommender systems.\\n\\nOverall, LangChain has the potential to simplify the process of building applications that utilize large language models, making it easier for developers to focus on building innovative applications that leverage the power of LLMs.', additional_kwargs={}, response_metadata={})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Integrate this concepts our chatbot"
      ],
      "metadata": {
        "id": "fnts-zHEskTy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_groq import ChatGroq\n",
        "from google.colab import userdata\n",
        "from langchain_core.messages import SystemMessage,HumanMessage,AIMessage\n",
        "\n",
        "API_KEY = userdata.get('GROQ_API_KEY')\n",
        "\n",
        "chat = ChatGroq(model_name='llama-3.3-70b-versatile',api_key=API_KEY,temperature=0)\n",
        "\n",
        "chat_history = [\n",
        "    SystemMessage(content=\"You are a helpful assistant\")\n",
        "]\n",
        "\n",
        "while True:\n",
        "  user_input = input('You: ')\n",
        "  ## save user chats\n",
        "  chat_history.append(HumanMessage(content=user_input))\n",
        "  if user_input == 'exit':\n",
        "    break\n",
        "  result = chat.invoke(user_input)\n",
        "  # save ai chats\n",
        "  chat_history.append(AIMessage(result.content))\n",
        "  print('AI: ',result.content)\n",
        "\n",
        "\n",
        "print(\"chat_history saved: \",chat_history)"
      ],
      "metadata": {
        "id": "5WeKMAcUrQzJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40f33fa7-3680-4dac-878a-c5d8c5b536e1"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You: hi\n",
            "AI:  It's nice to meet you. Is there something I can help you with or would you like to chat?\n",
            "You: hello\n",
            "AI:  Hello. It's nice to meet you. Is there something I can help you with or would you like to chat?\n",
            "You: exit\n",
            "chat_history saved:  [SystemMessage(content='You are a helpful assistant', additional_kwargs={}, response_metadata={}), HumanMessage(content='hi', additional_kwargs={}, response_metadata={}), AIMessage(content=\"It's nice to meet you. Is there something I can help you with or would you like to chat?\", additional_kwargs={}, response_metadata={}), HumanMessage(content='hello', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hello. It's nice to meet you. Is there something I can help you with or would you like to chat?\", additional_kwargs={}, response_metadata={}), HumanMessage(content='exit', additional_kwargs={}, response_metadata={})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Concept of Dynamic Message\n",
        "\n",
        "#### Using `ChatPromptTemplate` for list of messages and you want to make message be dynamic."
      ],
      "metadata": {
        "id": "tIStBfXat9hI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.messages import SystemMessage,HumanMessage\n",
        "\n",
        "chat_template = ChatPromptTemplate([\n",
        "    SystemMessage(content=\"You are a helpful {domain} expert\"),\n",
        "    HumanMessage(content='Explain in simple term, what is {topic}')\n",
        "])\n",
        "\n",
        "prompt = chat_template.invoke({'domain':\"math\",\"topic\":\"Derivatives\"})\n",
        "\n",
        "print(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h2anzhNutFIZ",
        "outputId": "24ef0ac0-c257-4cff-ccc8-ca88e2e67375"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "messages=[SystemMessage(content='You are a helpful {domain} expert', additional_kwargs={}, response_metadata={}), HumanMessage(content='Explain in simple term, what is {topic}', additional_kwargs={}, response_metadata={})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Problem : you noticed that we set domain 'math' and topic 'derivatives' but when we print prompt then shows {domain} and {topic} means not applied invoke. this is wiered , at this problem we use second method two shows desired prompts."
      ],
      "metadata": {
        "id": "zuNTY6VPvO31"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.messages import SystemMessage,HumanMessage\n",
        "\n",
        "chat_template = ChatPromptTemplate([\n",
        "  ## second method\n",
        "  (\"system\",\"you are a helpful {domain} expert\"),\n",
        "  (\"human\",\"Explain in simple term, what is {topic}\")\n",
        "])\n",
        "\n",
        "prompt = chat_template.invoke({'domain':\"math\",\"topic\":\"Derivatives\"})\n",
        "\n",
        "print(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_3UFjvhu_2L",
        "outputId": "913a741a-3308-4663-aae0-b81802b94612"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "messages=[SystemMessage(content='you are a helpful math expert', additional_kwargs={}, response_metadata={}), HumanMessage(content='Explain in simple term, what is Derivatives', additional_kwargs={}, response_metadata={})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Note: Above you noticed that here we get desired message successfully !!"
      ],
      "metadata": {
        "id": "ZmjSWW5rwHgK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Message Placeholder\n",
        "\n",
        "### A **Message PlaceHolder** in LangChain is a special placeholder inside a ChatPromptTemplate to dynamically insert chat history or a list of messages at runtime."
      ],
      "metadata": {
        "id": "kc63e022wWQB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "\n",
        "## chat template\n",
        "\n",
        "chat_template = ChatPromptTemplate([\n",
        "    (\"system\",\"you are a helpful customer support agent\"),\n",
        "    ## add message place holder\n",
        "    MessagesPlaceholder(variable_name='chat_history'),\n",
        "    (\"human\",\"{query}\")\n",
        "])\n",
        "\n",
        "\n",
        "chat_history = []\n",
        "\n",
        "## load chat history\n",
        "with open('/content/chat_history.txt') as f:\n",
        "  chat_history.extend(f.readlines())\n",
        "\n",
        "#print(chat_history)\n",
        "\n",
        "## create prompt\n",
        "prompt = chat_template.invoke({'chat_history':chat_history,'query':HumanMessage('where is my refund.')})\n",
        "\n",
        "res = chat.invoke(prompt)\n",
        "print(res.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r2IpUTnywBPE",
        "outputId": "f3829fc4-0366-4fdb-9850-d752beb2f91e"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I apologize for the delay in your refund. I've checked on the status of your refund for order #12345, and it's still being processed. As previously mentioned, refunds typically take 3-5 business days to complete. If it's been more than 5 business days, please allow me to look into this further. Can you please confirm your payment method for the order so I can investigate?\n"
          ]
        }
      ]
    }
  ]
}