{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN1Q7yOvWEuBpMOSi0MoMGZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/singhraj00/langchain-tutorial/blob/main/Langchain_Runnables.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Langchain Aam-Jindagi"
      ],
      "metadata": {
        "id": "mJtlVHnhSQv7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### we have create an fake llm, prompt class and after connect both them."
      ],
      "metadata": {
        "id": "lFdk7p6rSY2-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ujEw5zSSC7y"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "class FakeLLM:\n",
        "\n",
        "  def __init__(self):\n",
        "    print('LLM Created')\n",
        "\n",
        "  def predict(self,prompt):\n",
        "    response_list=[\n",
        "        \"delhi is the capital of india.\",\n",
        "        \"IPL is a t20 league.\",\n",
        "        \"AI stands for artificial intelligence\"\n",
        "    ]\n",
        "    return {'response':random.choice(response_list)}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## test fake llm"
      ],
      "metadata": {
        "id": "_oepHtZkTAIS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = FakeLLM()\n",
        "llm.predict('what is the capital of india?')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XaJjOHzdS-dO",
        "outputId": "2b29aaa6-859b-4c8f-c22a-eb2b1f575eaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LLM Created\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'response': 'delhi is the capital of india.'}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## craete an fake prompt"
      ],
      "metadata": {
        "id": "Q_teXBvWTKrf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FakePrompt:\n",
        "  def __init__(self,template,input_variables):\n",
        "    self.template=template\n",
        "    self.input_variables=input_variables\n",
        "\n",
        "  def format(self,input_dict):\n",
        "    return self.template.format(**input_dict)"
      ],
      "metadata": {
        "id": "ALnqF65pTG7_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## test fake prompt"
      ],
      "metadata": {
        "id": "8xGasoMwTlBN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "template=FakePrompt(\n",
        "    template=\"write an {length} poem about {topic}\",\n",
        "    input_variables=['length','topic']\n",
        ")\n",
        "\n",
        "prompt = template.format({\n",
        "   'length':'short',\n",
        "    'topic':'AI'\n",
        "})\n",
        "prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "BVp1s6aQTjXG",
        "outputId": "a45f7915-b343-484c-c5eb-6c0a466e8a23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'write an short poem about AI'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm = FakeLLM()\n",
        "llm.predict(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7RpzPxBjT7NP",
        "outputId": "2d105336-61c1-4996-b3e1-1124ac704657"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LLM Created\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'response': 'IPL is a t20 league.'}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Combined Them For Create an FAKELLMCHAIN"
      ],
      "metadata": {
        "id": "TrMaZ-I9Ud9h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FakeLLMChain:\n",
        "  def __init__(self,llm,prompt):\n",
        "    self.llm=llm\n",
        "    self.prompt=prompt\n",
        "\n",
        "  def run(self,input_dict):\n",
        "      final_prompt = self.prompt.format(input_dict)\n",
        "      result = self.llm.predict(final_prompt)\n",
        "      return result['response']"
      ],
      "metadata": {
        "id": "R40OJbV1Ubpe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Chain"
      ],
      "metadata": {
        "id": "s77T5votU8sg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "template=FakePrompt(\n",
        "    template=\"write an {length} poem about {topic}\",\n",
        "    input_variables=['length','topic']\n",
        ")"
      ],
      "metadata": {
        "id": "4BwaWF8RVRZI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = FakeLLM()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_X_ArtR_VS-y",
        "outputId": "4fa17fd7-4983-425c-f906-d6c965b74531"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LLM Created\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain = FakeLLMChain(llm,template)\n",
        "chain.run({\n",
        "    'length':'short',\n",
        "    'topic':'AI'\n",
        "})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "ou5oGZN3U4Kj",
        "outputId": "17b361b7-78ab-4dd0-8750-5e6ee364dbe5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'IPL is a t20 league.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Langchain Mentos--Jindagi\n",
        "\n",
        "### concepts of Runnable Class\n",
        "\n",
        "### Runnable is nothing , it's just an abstract class, whivh used to builds every componts in lagchain."
      ],
      "metadata": {
        "id": "VxdOACoMVl29"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from abc import ABC, abstractmethod\n",
        "\n",
        "class Runnable(ABC):\n",
        "    @abstractmethod\n",
        "    def invoke(self,input):\n",
        "        pass"
      ],
      "metadata": {
        "id": "uxzClhQMVAQk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Re-Create FakeLLM Using Runnable"
      ],
      "metadata": {
        "id": "N5PdS2JdWH9J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FakeLLM(Runnable):\n",
        "  def __init__(self):\n",
        "    print('llm created')\n",
        "\n",
        "  def invoke(self,prompt):\n",
        "    response_list=[\n",
        "        \"delhi is the capital of india.\",\n",
        "        \"IPL is a t20 league.\",\n",
        "        \"AI stands for artificial intelligence\"\n",
        "    ]\n",
        "    return {'response':random.choice(response_list)}\n",
        "\n",
        "    ## previous - in aam-jindagi\n",
        "    def predict(self,prompt):\n",
        "      response_list=[\n",
        "        \"delhi is the capital of india.\",\n",
        "        \"IPL is a t20 league.\",\n",
        "        \"AI stands for artificial intelligence\"\n",
        "     ]\n",
        "      return {'response':random.choice(response_list)}\n"
      ],
      "metadata": {
        "id": "_sase9kwWGlg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Re-Create FakePrompt Class"
      ],
      "metadata": {
        "id": "OLITrPQfWmsD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class FakePrompt(Runnable):\n",
        "  def __init__(self,template,input_variables):\n",
        "    self.template=template\n",
        "    self.input_variables=input_variables\n",
        "\n",
        "  ## mentos - jindagi\n",
        "\n",
        "  def invoke(self,input_dict):\n",
        "    return self.template.format(**input_dict)\n",
        "\n",
        "  ## aam - jindagi\n",
        "  def format(self,input_dict):\n",
        "    return self.template.format(**input_dict)\n",
        "\n"
      ],
      "metadata": {
        "id": "1ys72mt-WgaP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create FakeStringOutputParser Class"
      ],
      "metadata": {
        "id": "rHCvA6EhW5np"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FakeStrOutputParser(Runnable):\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def invoke(self,input_data):\n",
        "    return input_data['response']"
      ],
      "metadata": {
        "id": "8FSBgiwiW3iR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create an class RunnableConnector (LLMChain)"
      ],
      "metadata": {
        "id": "5gw9ZmwuXKMQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FakeRunnableConnector(Runnable):\n",
        "  def __init__(self,runnable_list):\n",
        "    self.runnable_list = runnable_list\n",
        "\n",
        "  def invoke(self,input_data):\n",
        "    ## sequential chain\n",
        "    for runnable in self.runnable_list:\n",
        "      input_data = runnable.invoke(input_data)\n",
        "    return input_data\n"
      ],
      "metadata": {
        "id": "a1pzSYXFXJE8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template = FakePrompt(\n",
        "    template=\"write an {length} poem about {topic}\",\n",
        "    input_variables=['length','topic']\n",
        ")"
      ],
      "metadata": {
        "id": "JMs09KoKXfz0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = FakeLLM()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0tCLVzfXljr",
        "outputId": "590999a9-cd85-409a-f067-6022f8b3a8dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "llm created\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain = FakeRunnableConnector([template,llm,FakeStrOutputParser()])\n",
        "chain.invoke({\n",
        "    'length':'short',\n",
        "    'topic':'AI'\n",
        "})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "8IjyGXEXXmp4",
        "outputId": "8e126401-d0ee-403a-d2b1-e6f6abaedee4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'delhi is the capital of india.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A Chain is also Runnable , if you want to also combined then two chains between them."
      ],
      "metadata": {
        "id": "iKupiNyQX_Yo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "template1= FakePrompt(\n",
        "    template='write me a joke about {topic}',\n",
        "    input_variables=['topic']\n",
        ")\n",
        "template2= FakePrompt(\n",
        "    template='Explain the following joke {response}',\n",
        "    input_variables=['response']\n",
        ")"
      ],
      "metadata": {
        "id": "PEp0sJ2GXvxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = FakeLLM()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hmuVMPzsYYlI",
        "outputId": "38edcea4-b776-4e9d-ddbb-a2b1b25e9eba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "llm created\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "parser = FakeStrOutputParser()"
      ],
      "metadata": {
        "id": "g8pRo1ZHYhig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain1 = FakeRunnableConnector([template1,llm])\n",
        "chain2 = FakeRunnableConnector([template2,llm,parser])"
      ],
      "metadata": {
        "id": "GXjS0_L9Yjpn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_chain = FakeRunnableConnector([chain1,chain2])"
      ],
      "metadata": {
        "id": "RPLUfWtZYnWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_chain.invoke({'topic':'ipl'})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "B5mfjsfeYqMF",
        "outputId": "628eb99f-05d8-48b7-a6d6-e73850b33522"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'delhi is the capital of india.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Runnables In LangChain"
      ],
      "metadata": {
        "id": "xmXAY_WoaNsz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Types Of Runnables\n",
        "- Task-Specific Runnables\n",
        "- Runnable-Primitives"
      ],
      "metadata": {
        "id": "-Nld6OraaUfF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task-Specific Runnables\n",
        "\n",
        "### Definitions : These are core LangChain components that have been converted into runnables so they can be used in pipelines.\n",
        "\n",
        "### Purpose: Perform task-specific operation like LLM calls, prompting, retrieval etc.\n",
        "\n",
        "### Examples:\n",
        "- ChatOpenAI - Runs as LLM Model\n",
        "- PromptTemplate - Formats prompts dynamically\n",
        "- Retrievar - Retrieves relevant documents\n",
        "\n",
        "## Runnable Primitives\n",
        "\n",
        "### Definitios: These are fundamentals building blocks for structuring execution logic in AI workflows.\n",
        "\n",
        "### Purpose: They help orchestrate execution by defining how different runnables interact (sequentially, in parallel, conditionally etc.)\n",
        "\n",
        "### Examples:\n",
        "- RunnableSequence\n",
        "- RunnableParallel\n",
        "- RunnableBranch\n",
        "- RunnablePassthroug\n",
        "- Ru8nnableLambda"
      ],
      "metadata": {
        "id": "i4Dj0Ab6acln"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RunnableSequence"
      ],
      "metadata": {
        "id": "yeypWSImbyVT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RunnableSequence is a sequential chain of runnables in LangChain that executes each step one after another, passing the output of one step the input as next. it is useful whenyou need to compose multiple runnables together in a structuring workflow."
      ],
      "metadata": {
        "id": "QQb9fOAQb2OK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code Example"
      ],
      "metadata": {
        "id": "sKPMMDtccN7W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_groq import ChatGroq\n",
        "from google.colab import userdata\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain.schema.runnable import RunnableSequence\n",
        "\n",
        "API_KEY = userdata.get('GROQ_API_KEY')\n",
        "\n",
        "model = ChatGroq(model_name='llama-3.3-70b-versatile',api_key=API_KEY,temperature=0)\n",
        "\n",
        "template = PromptTemplate(\n",
        "    template='write me a joke about {topic}',\n",
        "    input_variables = ['topic']\n",
        ")\n",
        "\n",
        "parser = StrOutputParser()\n",
        "\n",
        "chain = RunnableSequence(template,model,parser)\n",
        "\n",
        "print(chain.invoke({'topic':'cricket'}))\n"
      ],
      "metadata": {
        "id": "G2dKqVBDaQca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f63779d-d1ca-4a38-ab46-f49815c45df3"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Why did the cricket go to the doctor?\n",
            "\n",
            "Because it had a bug in its system, and it wasn't just a googly.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code Example 2"
      ],
      "metadata": {
        "id": "KLP-zUQpemR3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_groq import ChatGroq\n",
        "from google.colab import userdata\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain.schema.runnable import RunnableSequence\n",
        "\n",
        "API_KEY = userdata.get('GROQ_API_KEY')\n",
        "\n",
        "model = ChatGroq(model_name='llama-3.3-70b-versatile',api_key=API_KEY,temperature=0)\n",
        "\n",
        "template1 = PromptTemplate(\n",
        "    template='write me a joke about {topic}',\n",
        "    input_variables = ['topic']\n",
        ")\n",
        "\n",
        "template2 = PromptTemplate(\n",
        "    template='Explain the following joke in {response}',\n",
        "    input_variables = ['response']\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "parser = StrOutputParser()\n",
        "\n",
        "chain1 = RunnableSequence(template1,model,parser)\n",
        "chain2 = RunnableSequence(template2,model,parser)\n",
        "\n",
        "final_chain = RunnableSequence(chain1,chain2)\n",
        "\n",
        "print(final_chain.invoke({'topic':'cricket'}))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-nJSBWJddu89",
        "outputId": "bbaaf799-0d16-432f-9dc5-eff11b79515f"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A clever joke. Let's break it down:\n",
            "\n",
            "The joke starts by setting up a familiar scenario: \"Why did the cricket go to the doctor?\" This implies that the cricket is ill or has a problem.\n",
            "\n",
            "The punchline is: \"Because it had a bug in its system, and it wasn't just a googly.\"\n",
            "\n",
            "Here, \"bug in its system\" is a play on words. In computing, a \"bug\" refers to an error or a glitch in a system. However, in this joke, \"bug\" also refers to an insect, which is a clever connection to the fact that the subject is a cricket (an insect).\n",
            "\n",
            "The second part of the punchline, \"and it wasn't just a googly,\" is a reference to the sport of cricket. In cricket, a \"googly\" is a type of delivery (a ball thrown by the bowler) that is designed to deceive the batsman. It's a clever and tricky delivery.\n",
            "\n",
            "So, the joke is saying that the cricket went to the doctor because it had a bug (insect) in its system, but it's not just a simple \"googly\" (a tricky delivery in cricket). The joke relies on the double meaning of \"bug\" and the reference to cricket terminology to create a clever and humorous connection.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RunnableParallel\n",
        "\n",
        "### The RunnableParallel object allows us to define multiple values and operations, and run them all in parallel."
      ],
      "metadata": {
        "id": "Cy1Oen20g3Ht"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code Example\n",
        "\n",
        "### We want to generate an twitter or linkedin post on specific topic in parallel."
      ],
      "metadata": {
        "id": "JFQQ_n32hRe3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_groq import ChatGroq\n",
        "from google.colab import userdata\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain.schema.runnable import RunnableParallel,RunnableSequence\n",
        "\n",
        "API_KEY = userdata.get('GROQ_API_KEY')\n",
        "\n",
        "model = ChatGroq(model_name='llama-3.3-70b-versatile',api_key=API_KEY,temperature=0)\n",
        "\n",
        "template1 = PromptTemplate(\n",
        "    template='generate an tweet about {topic}',\n",
        "    input_variables = ['topic']\n",
        ")\n",
        "\n",
        "template2 = PromptTemplate(\n",
        "    template='generate an linkedin post about {topic}',\n",
        "    input_variables = ['topic']\n",
        ")\n",
        "\n",
        "parser = StrOutputParser()\n",
        "\n",
        "parallel_chain= RunnableParallel({\n",
        "    \"tweet\":RunnableSequence(template1,model,parser),\n",
        "    \"linkedin\":RunnableSequence(template2,model,parser)\n",
        "})\n",
        "\n",
        "result = parallel_chain.invoke({'topic':'AI'})\n",
        "\n",
        "print('Tweet: ',result['tweet'])\n",
        "print('Linkedin: ',result['linkedin'])\n",
        "\n",
        "\n",
        "final_chain.get_graph().print_ascii()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRVxbbRyg2jp",
        "outputId": "d75f825d-27f2-447c-84de-87a3bba8bbb6"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tweet:  \"AI is revolutionizing the world, from healthcare to finance, and beyond. What are your thoughts on the future of artificial intelligence? Will it enhance or disrupt our daily lives? #AI #ArtificialIntelligence #FutureTech\"\n",
            "Linkedin:  **The Future of Work: How AI is Revolutionizing Industries**\n",
            "\n",
            "As we continue to navigate the ever-changing landscape of technology, one thing is clear: Artificial Intelligence (AI) is no longer just a buzzword, but a reality that's transforming the way we work and live.\n",
            "\n",
            "From automating mundane tasks to analyzing complex data sets, AI is unlocking new levels of efficiency, productivity, and innovation across industries. Whether it's in healthcare, finance, or marketing, AI is helping professionals make better decisions, faster and with greater accuracy.\n",
            "\n",
            "But what does this mean for the future of work? As AI takes on more routine and repetitive tasks, it's freeing up human talent to focus on high-value tasks that require creativity, empathy, and problem-solving skills.\n",
            "\n",
            "**Key takeaways:**\n",
            "\n",
            "1. **Upskilling and reskilling**: As AI assumes more routine tasks, it's essential to invest in developing skills that complement AI, such as critical thinking, creativity, and emotional intelligence.\n",
            "2. **Augmenting human capabilities**: AI is not here to replace humans, but to augment our capabilities, enabling us to achieve more with less effort and greater precision.\n",
            "3. **New job opportunities**: The rise of AI is creating new job opportunities in fields like AI development, deployment, and maintenance, as well as in areas like data science, analytics, and cybersecurity.\n",
            "\n",
            "**What are your thoughts on the impact of AI on the future of work? Share your insights and experiences in the comments below!**\n",
            "\n",
            "Let's continue the conversation and explore the vast possibilities that AI has to offer. #AI #FutureOfWork #Innovation #Technology #DigitalTransformation\n",
            "                 +-------------+                 \n",
            "                 | PromptInput |                 \n",
            "                 +-------------+                 \n",
            "                        *                        \n",
            "                        *                        \n",
            "                        *                        \n",
            "                +----------------+               \n",
            "                | PromptTemplate |               \n",
            "                +----------------+               \n",
            "                        *                        \n",
            "                        *                        \n",
            "                        *                        \n",
            "                  +----------+                   \n",
            "                  | ChatGroq |                   \n",
            "                  +----------+                   \n",
            "                        *                        \n",
            "                        *                        \n",
            "                        *                        \n",
            "               +-----------------+               \n",
            "               | StrOutputParser |               \n",
            "               +-----------------+               \n",
            "                        *                        \n",
            "                        *                        \n",
            "                        *                        \n",
            "      +----------------------------------+       \n",
            "      | Parallel<joke_gen,joke_exp>Input |       \n",
            "      +----------------------------------+       \n",
            "                ***            ***               \n",
            "              **                  ***            \n",
            "            **                       **          \n",
            "+----------------+                     **        \n",
            "| PromptTemplate |                      *        \n",
            "+----------------+                      *        \n",
            "          *                             *        \n",
            "          *                             *        \n",
            "          *                             *        \n",
            "    +----------+                        *        \n",
            "    | ChatGroq |                        *        \n",
            "    +----------+                        *        \n",
            "          *                             *        \n",
            "          *                             *        \n",
            "          *                             *        \n",
            "+-----------------+             +-------------+  \n",
            "| StrOutputParser |             | Passthrough |  \n",
            "+-----------------+             +-------------+  \n",
            "                ***            ***               \n",
            "                   **        **                  \n",
            "                     **    **                    \n",
            "      +-----------------------------------+      \n",
            "      | Parallel<joke_gen,joke_exp>Output |      \n",
            "      +-----------------------------------+      \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RunnablePassthrough\n",
        "\n",
        "### Runnable to passthrough inputs unchanged or with additional keys."
      ],
      "metadata": {
        "id": "sfmfbykzfK41"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_groq import ChatGroq\n",
        "from google.colab import userdata\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain.schema.runnable import RunnableSequence,RunnableParallel,RunnablePassthrough\n",
        "\n",
        "API_KEY = userdata.get('GROQ_API_KEY')\n",
        "\n",
        "model = ChatGroq(model_name='llama-3.3-70b-versatile',api_key=API_KEY,temperature=0)\n",
        "\n",
        "template1 = PromptTemplate(\n",
        "    template='write me a joke about {topic}',\n",
        "    input_variables = ['topic']\n",
        ")\n",
        "\n",
        "template2 = PromptTemplate(\n",
        "    template='Explain the following joke in {response}',\n",
        "    input_variables = ['response']\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "parser = StrOutputParser()\n",
        "\n",
        "\n",
        "chain1 = RunnableSequence(template1,model,parser)\n",
        "chain2 = RunnableParallel({\n",
        "    \"joke_gen\":RunnablePassthrough(),\n",
        "    \"joke_exp\":RunnableSequence(template2,model,parser)\n",
        "})\n",
        "\n",
        "\n",
        "final_chain = RunnableSequence(chain1,chain2)\n",
        "\n",
        "result = final_chain.invoke({'topic':'cricket'})\n",
        "\n",
        "print('Joke: ',result['joke_gen'])\n",
        "print('Explanation: ',result['joke_exp'])\n",
        "\n",
        "\n",
        "final_chain.get_graph().print_ascii()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-vJs9e_fDTw",
        "outputId": "9717e074-ac8a-4cdb-f7dd-b70cf3d5d546"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Joke:  Why did the cricket go to the doctor?\n",
            "\n",
            "Because it had a bug in its system, and it wasn't just a googly.\n",
            "Explanation:  A clever joke. Let's break it down:\n",
            "\n",
            "The joke starts by setting up a familiar scenario: \"Why did the cricket go to the doctor?\" This implies that the cricket is ill or has a problem.\n",
            "\n",
            "The punchline is: \"Because it had a bug in its system, and it wasn't just a googly.\"\n",
            "\n",
            "Here, \"bug in its system\" is a play on words. In computing, a \"bug\" refers to an error or a glitch in a system. However, in this joke, \"bug\" also refers to an insect, which is a clever connection to the fact that the subject is a cricket (an insect).\n",
            "\n",
            "The second part of the punchline, \"and it wasn't just a googly,\" is a reference to cricket (the sport). In cricket, a \"googly\" is a type of delivery bowled by a spin bowler, which turns in the opposite direction to what the batsman expects. The joke is saying that the cricket's problem (the \"bug\") is not just a tricky or unexpected situation (like a googly in cricket), but a real issue that requires medical attention.\n",
            "\n",
            "So, the joke relies on a double meaning of \"bug\" (insect and computer error) and a clever connection to the sport of cricket, making it a clever play on words.\n",
            "                 +-------------+                 \n",
            "                 | PromptInput |                 \n",
            "                 +-------------+                 \n",
            "                        *                        \n",
            "                        *                        \n",
            "                        *                        \n",
            "                +----------------+               \n",
            "                | PromptTemplate |               \n",
            "                +----------------+               \n",
            "                        *                        \n",
            "                        *                        \n",
            "                        *                        \n",
            "                  +----------+                   \n",
            "                  | ChatGroq |                   \n",
            "                  +----------+                   \n",
            "                        *                        \n",
            "                        *                        \n",
            "                        *                        \n",
            "               +-----------------+               \n",
            "               | StrOutputParser |               \n",
            "               +-----------------+               \n",
            "                        *                        \n",
            "                        *                        \n",
            "                        *                        \n",
            "      +----------------------------------+       \n",
            "      | Parallel<joke_gen,joke_exp>Input |       \n",
            "      +----------------------------------+       \n",
            "                ***            ***               \n",
            "              **                  ***            \n",
            "            **                       **          \n",
            "+----------------+                     **        \n",
            "| PromptTemplate |                      *        \n",
            "+----------------+                      *        \n",
            "          *                             *        \n",
            "          *                             *        \n",
            "          *                             *        \n",
            "    +----------+                        *        \n",
            "    | ChatGroq |                        *        \n",
            "    +----------+                        *        \n",
            "          *                             *        \n",
            "          *                             *        \n",
            "          *                             *        \n",
            "+-----------------+             +-------------+  \n",
            "| StrOutputParser |             | Passthrough |  \n",
            "+-----------------+             +-------------+  \n",
            "                ***            ***               \n",
            "                   **        **                  \n",
            "                     **    **                    \n",
            "      +-----------------------------------+      \n",
            "      | Parallel<joke_gen,joke_exp>Output |      \n",
            "      +-----------------------------------+      \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RunnableLambda\n",
        "\n",
        "### RunnableLambda converts a python callable into a Runnable.\n",
        "\n",
        "### CodeExample"
      ],
      "metadata": {
        "id": "vh5WylYFi5_5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_groq import ChatGroq\n",
        "from google.colab import userdata\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain.schema.runnable import RunnableParallel,RunnableLambda,RunnableSequence,RunnablePassthrough\n",
        "\n",
        "API_KEY = userdata.get('GROQ_API_KEY')\n",
        "\n",
        "model = ChatGroq(model_name='llama-3.3-70b-versatile',api_key=API_KEY,temperature=0)\n",
        "\n",
        "template = PromptTemplate(\n",
        "    template='write me a joke about {topic}',\n",
        "    input_variables = ['topic']\n",
        ")\n",
        "\n",
        "\n",
        "def count_word(text):\n",
        "  return len(text.split())\n",
        "\n",
        "parser = StrOutputParser()\n",
        "\n",
        "joke_gen_chain = RunnableSequence(template,model,parser)\n",
        "\n",
        "joke_exp_chain = RunnableParallel({\n",
        "    \"joke\":RunnablePassthrough(),\n",
        "    ## runnable lambda used to convert python function to runnable lambda\n",
        "    \"word_count\":RunnableLambda(count_word)\n",
        "    ## second way\n",
        "    # \"word_count\":(lambda x:len(x.split()))\n",
        "})\n",
        "\n",
        "\n",
        "final_chain = RunnableSequence(joke_gen_chain,joke_exp_chain)\n",
        "\n",
        "result = final_chain.invoke({'topic':'cricket'})\n",
        "\n",
        "print('joke: ',result['joke'])\n",
        "print('word count: ',result['word_count'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o1tQrMpcgums",
        "outputId": "401683fe-4c83-4d49-9b62-41f031904c60"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "joke:  Why did the cricket go to the doctor?\n",
            "\n",
            "Because it had a bug in its system, and it wasn't just a googly.\n",
            "word count:  22\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RunnableBranch\n",
        "\n",
        "### Runnable that selects which branch to run based on a condition."
      ],
      "metadata": {
        "id": "qoE3uQahkrCb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code Example"
      ],
      "metadata": {
        "id": "Woif6CBpkyQ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_groq import ChatGroq\n",
        "from google.colab import userdata\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain.schema.runnable import RunnableLambda,RunnableSequence,RunnablePassthrough,RunnableBranch\n",
        "\n",
        "API_KEY = userdata.get('GROQ_API_KEY')\n",
        "\n",
        "model = ChatGroq(model_name='llama-3.3-70b-versatile',api_key=API_KEY,temperature=0)\n",
        "\n",
        "template1 = PromptTemplate(\n",
        "    template='generate an detail report about {topic}',\n",
        "    input_variables = ['topic']\n",
        ")\n",
        "\n",
        "template2 = PromptTemplate(\n",
        "    template='Summarize the following report in {response}',\n",
        "    input_variables = ['response']\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "parser = StrOutputParser()\n",
        "\n",
        "report_gen_chain = RunnableSequence(template1,model,parser)\n",
        "\n",
        "branch_chain= RunnableBranch(\n",
        "    (lambda x: len(x.split())>200, RunnableSequence(template2,model,parser)),\n",
        "    RunnablePassthrough()\n",
        ")\n",
        "\n",
        "final_chain = RunnableSequence(report_gen_chain,branch_chain)\n",
        "\n",
        "result = final_chain.invoke({'topic':'Russia Vs Ukraine'})\n",
        "\n",
        "print('Report: ',result)\n",
        "\n",
        "final_chain.get_graph().print_ascii()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7dilqXErjg03",
        "outputId": "fda86687-0de6-4bdb-dbca-91535828218e"
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Report:  The conflict between Russia and Ukraine is a complex and ongoing issue that has been escalating since 2014. The crisis has its roots in a mix of historical, cultural, and economic factors, and has had significant implications for regional and global security. This report aims to provide an in-depth analysis of the conflict, including its causes, key events, and current situation, in order to understand the complexities of the issue and identify potential solutions. The report will examine the historical background of the conflict, the causes of the conflict, key events, and the current situation, as well as the humanitarian impact and international response, in order to provide a comprehensive understanding of the conflict and its implications.\n",
            "  +-------------+    \n",
            "  | PromptInput |    \n",
            "  +-------------+    \n",
            "          *          \n",
            "          *          \n",
            "          *          \n",
            "+----------------+   \n",
            "| PromptTemplate |   \n",
            "+----------------+   \n",
            "          *          \n",
            "          *          \n",
            "          *          \n",
            "    +----------+     \n",
            "    | ChatGroq |     \n",
            "    +----------+     \n",
            "          *          \n",
            "          *          \n",
            "          *          \n",
            "+-----------------+  \n",
            "| StrOutputParser |  \n",
            "+-----------------+  \n",
            "          *          \n",
            "          *          \n",
            "          *          \n",
            "    +--------+       \n",
            "    | Branch |       \n",
            "    +--------+       \n",
            "          *          \n",
            "          *          \n",
            "          *          \n",
            "  +--------------+   \n",
            "  | BranchOutput |   \n",
            "  +--------------+   \n"
          ]
        }
      ]
    }
  ]
}